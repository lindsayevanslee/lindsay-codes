[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog acts as a Joycean stream-of-consciousness record of the random tech-y things I’m experimenting with and learning. It started because I bought this domain name on a whim and thought it was too cool to not do something with. The purpose is to prevent things falling out of my goldfish brain. If you’re reading this, I hope it benefits you too, though there’s certainly no guarantee.\nPlease visit my website or LinkedIn for a slightly more professional persona."
  },
  {
    "objectID": "posts/apple-watch/index.html",
    "href": "posts/apple-watch/index.html",
    "title": "Analyze health data from the Apple Watch",
    "section": "",
    "text": "I have a doctor’s appointment soon, and I wanted to see if health data from my Apple Watch could help inform our conversation. The Health app has interesting visualizations out of the box, but I wanted to do something a tad more complicated.\nTo download the data from the Apple Health app on your iPhone, go to the Health app, click on your profile picture in the top right corner, and select “Export All Health Data”. This will create a zip file that you can share with yourself. Next download the zip file and extract the contents. The important file in this extract is export.xml. I have this file saved in a folder data/apple_health_export in my GitHub repository. I have the data folder in the .gitignore file so that the data is not uploaded to this GitHub repository.\nIt’s not super easy to work with the data in xml format, so I wanted to convert this data to csv format. Luckily I found someone else’s code to do exactly this. I saved their parsing function as convert_xml_to_csv.py in my GitHub repository for this analysis.\nTo see the xml data as json within the terminal, run the following command in the Terminal:\npython convert_xml_to_csv.py data/apple_health_export/export.xml\nTo convert the xml data to a csv file, run the following command in the Terminal:\npython convert_xml_to_csv.py data/apple_health_export/export.xml | jq -r '[.startDate, .endDate, .type, .unit, .value] | @csv' &gt; data/apple_health_export/export.csv\nThis requires the JSON parser jq to be installed. I installed jq via Homebrew on my Mac with: brew install jq.\nWith the data in a usable format, it’s easy to make all kinds of nice graphs. I have a couple examples in my script print_spo2_graph.R in the repository. The pdf output is saved to an output folder which is also in .gitignore (sorry, nosy nellies!)\nNow I’m all ready for my appointment. I’m sure my doctor will think this is useful and not a waste of everyone’s time."
  },
  {
    "objectID": "posts/shinylive/index.html",
    "href": "posts/shinylive/index.html",
    "title": "Going live with Shinylive",
    "section": "",
    "text": "I had a meeting at work recently where someone described a long, tedious process her team had to go through to write up a specifically-formatted report on a regular basis. She asked for help finding ways to reduce the time it takes to write these documents, namely by automating the sections that are fairly rote.\nOne option I thought of immediately was to build a Shiny app. I was thinking the user could input some key data points into the app, and then the app would analyze that data and produce a Word doc in a particular format based on the results.\nI’ve dabbled in Shiny before, but one struggle was figuring out how to host it. You could use a service like shinyapps.io, but using the free version isn’t very enterprise-friendly, and the non-free version is, well, not free. Then as if the gods were smiling down upon me, I came across a post in LinkedIn about shinylive, which is a flavor of shiny that runs completely in your web browser. In theory it would be possible to “host” a shinylive application on a Quarto website hosted on GitHub Pages completely for free. What better way to test this idea than on this very Quarto website hosted on GitHub Pages?\nI started off by following the instructions for the Quarto extension for shinylive. I installed the shinylive python package with:\n\npip install shinylive --upgrade\n\n…and added the extension to my quarto project (the repo for this website) with:\n\nquarto add quarto-ext/shinylive\n\nIn the YAML header for this post I added:\n\nfilters:\n  - shinylive\n\nThen the code for your shinylive application should all live in one code chunk marked with {shinylive-python} and containing #| standalone: true. This “standalone” bit apparently tells the code this chunk is a complete Shiny application instead of one spread across multiple files or chunks in the document, which they luckily say will be supported in the future.\nI wanted my shinylive application to have a download button that allows the user to download a docx file. I found the python package python-docx which can accomplish this. Next I had to figure out how to get shinylive to install and use this package. The documentation and examples say you can create a requirements.txt file. But when I tried to create a requirements.txt file in the same folder as my quarto file, the application wouldn’t work because python-docx wasn’t being installed, indicating the requirements were not being recognized. After a lot of digging I came across this GitHub issue that showed how you can get your quarto-based shinylive application to recognize other files by creating them dynamically in the same code chunk using a format like:\n\n## file: requirements.txt\npython-docx\nlxml\n\nReading back at the quarto extension documentation, I see they also explain how to do this there. If only I had originally thought to scroll down.\nTo create my actual prototype shinylive app, I followed the patterns in the examples for dynamically creating a file and allowing the user to download it. I also referred to the python-docx example code for how to create a Word doc with this package. Testing my app in the shinylive examples play area was helpful for debugging, which allowed me to removing variables like the fact that I’m rendering specifically in a Quarto site, that I’m publishing to GitHib Pages, etc.\nWithout further ado, here is my gorgeous and perfect app! You can see the code in the repo for this site.\n#| standalone: true\n\nimport io\nimport docx\nfrom shiny import App, render, ui\n\napp_ui = ui.page_fluid(\n    ui.input_slider(\"n\", \"How did the student do on the test?\", 0, 100, 40),\n    ui.download_button(\"download2\", \"Download\")\n)\n\ndef server(input, output, session):\n\n    @render.download(filename=\"file.docx\")\n    def download2():\n        #create document object\n        doc = docx.Document()\n\n        #add content to doc -------\n\n        #add a title\n        doc.add_heading('This is a report', 0)\n\n        #add a paragraph\n        p = doc.add_paragraph('A plain paragraph having some ')\n        p.add_run('bold').bold = True\n        p.add_run(' and some ')\n        p.add_run('italic.').italic = True\n\n        #add a heading\n        doc.add_heading('Results', level=1)\n\n        #add some lists\n        doc.add_paragraph(\n            'first item in unordered list', style='List Bullet'\n        )\n        doc.add_paragraph(\n            'first item in ordered list', style='List Number'\n        )\n\n        #define a function that outputs a string based on the value of input.n()\n        def get_interpretation(score):\n            if score &lt; 50:\n                return \"They did poorly\"\n            elif score &lt; 70:\n                return \"They did okay\"\n            elif score &lt; 90:\n                return \"They did well\"\n            else:\n                return \"They did excellent\"\n        \n        #add a table, pulling from value inputted by user\n        records = (\n            ('Score 1', input.n() , get_interpretation(input.n())),\n        )\n\n        \n        table = doc.add_table(rows=1, cols=3)\n        hdr_cells = table.rows[0].cells\n        hdr_cells[0].text = 'Score name'\n        hdr_cells[1].text = 'Score value'\n        hdr_cells[2].text = 'Interpretation'\n        \n        for score_name, score_value, score_desc in records:\n            row_cells = table.add_row().cells\n            row_cells[0].text = score_name\n            row_cells[1].text = str(score_value)\n            row_cells[2].text = score_desc\n\n        #add a page break\n        doc.add_page_break()\n\n\n        #save document ------------------\n        with io.BytesIO() as buf:\n            doc.save(buf)\n            yield buf.getvalue()\n\n\napp = App(app_ui, server)\n\n\n## file: requirements.txt\npython-docx\nlxml\n\nOne significant drawback with this so far is like I’ve already mentioned, your app has to be in one big code chunk, at least if you are embedding your shinylive app in Quarto. This limits how complex your app can be, which may be an issue in my particular use case. Another issue for me is that for data security reasons, my end users may want something that they can install and render on their own local computer without use of the internet. I’ve tried some light googling to see if it’s easy to create a desktop app from shiny and it doesn’t look so easy.\nSo we’ll see if this actually ends up being useful for the user I had in mind, but even if it isn’t, it was cool to learn anyway. Another fun python fact I learned: to force a tuple of tuples to remain a tuple of tuples even if it’s only got one tuple in it, slap a comma at the end of your tuple like so:\n\nrecords = (\n    ('Score 1', input.n() , get_interpretation(input.n())), #this last comma is key\n)\n\nLessons abound!"
  },
  {
    "objectID": "posts/library-of-congress-download/index.html",
    "href": "posts/library-of-congress-download/index.html",
    "title": "Downloading newspapers from the Library of Congress",
    "section": "",
    "text": "I’ve been working on a genealogy project for several months now (trying to find the baby daddy of someone many generations ago, juicy stuff but the details aren’t important). One incredibly useful and fascinating resource has been old newspaper articles, which I’ve accessed primarily through Newspapers.com. While this site is an archival treasure trove, they don’t yet have papers available in the counties where my ancestor lived, namely Fentress and Overton Counties in Tennessee. I’m almost positive that if I did have access to papers from those areas, my family would appear, and I would be one step closer to solving this family mystery.\nI decided to look one day to see if there were any other archives that had papers from these areas, and I found that the Library of Congress (duh, I should have known) has an unbelievable directory of US newspapers, and it’s searchable by county and year. I found many newspapers on this list that aren’t on Newspapers.com, but only one that is fully digitized and available on the Library of Congress website. The others are on microfilm at the Tennessee State Library, which I have other big plans for, but for now: while this digitized paper was in a town that isn’t directly relevant to my family, I thought it would still be a fun project to download all the pages and attempt to run some OCR to look for any mention of my family’s name.\nThe newspaper I attempted to download is the The Rugbeian and District Reporter out of Rugby, TN. This town was apparently founded as an “experimental utopian colony” in 1880, where the “second sons” of England could be free to own land and live life as they wished. It was plagued by disorganization and typhoid from the start, so the town–and this newspaper–didn’t last very long.\nOn the Library of Congress website, each page of the paper has its own URL, and you can cycle through all pages of an issue and all issues of a paper by clicking through arrow buttons. You can also download the pages in multiple formats. I wanted to download the PDF version and the “OCR ALTO” version, which I came to learn is an xml format denoting the layout of text on an image and, in my case, the actual text present in the image. My goal was to write scripts that would:\n\ncycle through all issues and all the pages of each issue\ndownload the PDF and OCR ALTO format for each image\nextract the text from each OCR ALTO file\n\nWith a heavy assist from Claude, I accomplished this in two scripts: download_pages.py for steps 1 and 2 and transcribe_pages.py for step 3, available in my GitHub repository.\nThe main function in download_pages.py that kicks off the algorithm is download_newspaper_pages(), which takes one parameter: the URL of the first page of the first issue of the newspaper that you want to download. It uses Selenium and Google Chrome to perform the webscraping algorithm. Selenium would need to be installed first before this function would work, and I’ll leave those instructions for elsewhere (see my first post for another example where I used Selenium).\nThis file also includes several helper functions that also extract the metadata for each issue from the site, sanitize filenames, and handle technical difficulties. Often times the Library of Congress site would get stuck and send you to a “we’re having technical difficulties” page, which often would be resolved if the page was just refreshed again. So a big part of the error handling is checking to make sure this page didn’t appear, and if it did to refresh the page until the publication page rendered again.\nThe pages are extracted into a downloads folder in the same directory as the script, and structured like:\n\ndownloads/\n├── Publication_Title_1/\n│   ├── date_1/\n│   │   ├── page_1.pdf\n│   │   ├── page_1.xml\n│   │   ├── page_2.pdf\n│   │   ├── page_2.xml\n│   │   └── metadata.json\n│   ├── date_2/\n│   │   ├── page_1.pdf\n│   │   ├── page_1.xml\n│   │   ├── page_2.pdf\n│   │   ├── page_2.xml\n│   │   └── metadata.json\n│   └── ...\n\nAfter running this algorithm to extract all the image files, next the transcribe_pages.py file can be run to pull text out of the OCR ALTO xml files. When I started this project I assumed that this file only gave the layout of the document but not the actual text itself, meaning I’d have to perform my own OCR algorithm. But actually this file actually has the OCR already completed, and all I needed to do was extract the text from each page.\nThe main function in this script is extract_all_text_from_alto() with one parameter: the path to the folder for the publication (downloads/Publication_Title_1 in the example above). This script then cycles through all of the pages in this folder and generates a new text file for each page, so now there will be a pdf, xml, and txt file for each page.\nAfter extracting all this text from all these pages, it was time to look for mention of my family. I did a classic Ctrl + F on the folder, searched for some key phrases, and found nothing. But I guess that’s research for ya.\nIf I can get my hands on those other microfilms, then we’ll really be cooking with gas, stay tuned."
  },
  {
    "objectID": "posts/get-started-with-conda/index.html",
    "href": "posts/get-started-with-conda/index.html",
    "title": "Get started with conda",
    "section": "",
    "text": "My python skills aren’t super strong yet, but they’re strong enough to know that environment management is key, and it’s an area I know next to nothing about. I figured it’s time to at least learn the basics.\nThere seem to be two main tools for environment management in python: conda and venv. I tried to do a little bit of research to see what the pros and cons are of each. This reddit thread was useful, and this blog post linked to from that thread was helpful too. It seems like the main differences are that venv’s main purpose is for isolating a virtual environment, while conda provides more help managing packages for your project. It appears conda helps you by installing commonly used packages, which may not be suitable in industry/production settings. I’m just playing around and need all the help I can get, so I decided to take a crack at conda first.\nThere are two flavors of conda: anaconda and miniconda. anaconda comes with a lot more packages and also an app to manage the packages. miniconda is more stripped-down. I decided to try miniconda first, because I didn’t like the idea of installing more apps on my computer.\nI’m on a Mac, and the savior for any amateur programmer on a Mac is Homebrew, a wonderful package manager for macOS which makes it so much easier to install all the little programs and utilities needed to code stuff. I can’t remember all the things I’ve installed with it, but it always seems to be there to save the day when I’m having issues installing python or R packages. Luckily it saves the day here again: you can install miniconda with Homebrew as well. This Medium article helped me figure out how. All I needed to do was run this in the Terminal:\n\nbrew install --cask miniconda\nconda init zsh\n\nThen restart the Terminal. I tested the install worked by running conda, which didn’t return any errors, which is what I wanted to see.\nNow to try our first environment! I used my inky-frame project as a test case (check out my post about it here). I first wanted to try the more point-and-click way to create a python environment in VS Code. It says by using the “Python: Create Environment” command from the Control Palette, it should automatically detect an existing requirements.txt file to install the packages you need to create the environment. I tried this first, but after trying to run a script in the new environment, I got “package not found” errors, meaning it didn’t seem to actually install the necessary packages listed in requirements.txt.\nNext I tried creating the conda environment using the command line by running:\n\nconda install --file requirements.txt\n\nBut this still didn’t seem to install the packages. It gave a long error message like this:\n\nChannels:\n - defaults\nPlatform: osx-64\nCollecting package metadata (repodata.json): done\nSolving environment: failed\n\nLibMambaUnsatisfiableError: Encountered problems while solving:\n  - nothing provides requested asttokens 2.2.1\n  - nothing provides requested charset-normalizer 3.1.0\n  ...\n\nCould not solve for environment specs\nThe following packages are incompatible\n├─ appnope 0.1.3  is installable and it requires\n│  └─ python &gt;=3.12,&lt;3.13.0a0 , which can be installed;\n├─ asttokens 2.2.1  does not exist (perhaps a typo or a missing channel);\n...\n\n(where ... means multiple lines of the same pattern for various other packages). I was stumped so I pasted this error into GitHub Copilot and asked what it meant. It told me I may need to add a “channel” for conda to search in; I only had the defaults channel added, which didn’t seem to have all the packages I was asking for. It suggested I add the conda-forge channel. I did this by running:\n\nconda config --add channels conda-forge\n\nAfter doing this, I tried to create the environment again, and I got a new but more promising error. This time it said there were only two packages it wasn’t able to install (pure-eval==0.2.2 and stack-data==0.6.2). It suggested searching https://anaconda.org to see if I could find a channel they were in that I could add. I tried searching but couldn’t find any channels they were in.\nLuckily from the conda documentation I saw that you can still install packages with pip while using conda. I wasn’t sure how to tell conda to use pip using a requirements.txt file, but it seemed like it was easier using a environment.yml file instead. I asked GitHub Copilot to help me convert my requirements.txt to a environment.yml file, then tried to create the environment again with:\n\nconda env create -f environment.yml\n\nI still got an error, but luckily it was a small one: I needed to use single = for specifying versions for the dependencies in environment.yml, while I needed to use == for specifying versions for packages installed with pip. Fixing this, I reran the command, and finally I created a working conda environment! I also retried the point-and-click way to create an environment in VS Code, it also worked, automatically using the environment.yml file to install the necessary packages.\nFor the moment I’m keeping both requirements.txt and environment.yml in my repo because requirements.txt is used in the GitHub Actions I have set up. I also noticed that my GitHub Actions use different python version than what I’m using locally in this new conda environment. But I’ll take a few moments to bask in my win and will worry about that later.\n\nUpdate 2024-11-10 - updating a conda environment\nI’ve discovered that unless you create an immaculate environment.yml on the first try, you’ll probably have to update your conda environment. You can do that like this:\n\nconda activate myenv\nconda env update --file environment.yml --prune\n\n\n\nUpdate 2025-02-09 - removing packages from a conda environment\nI’ve learned that the above command in my last update conda env update doesn’t remove any packages that you remove from your environment.yml file. In order to remove packages no longer needed, you need to delete and recreate your environment:\n\nconda deactivate\nconda env remove --name myenv\nconda env create --file environment.yml\nconda activate myenv"
  },
  {
    "objectID": "posts/realtime-bus-departure-board/index.html",
    "href": "posts/realtime-bus-departure-board/index.html",
    "title": "Making a custom bus departure sign",
    "section": "",
    "text": "I was basking in the glory of finishing my box office sign one day when my sister1 sent me this video of an English man making a business out of his passion for railway departure departure boards. I immediately thought, “this is so cool,” then thought maybe I can do something similar with our mediocre bus network2 here in Nashville using what I learned from the box office sign project. I don’t have a car and rely on this middling bus network to get around, so it would be quite useful to have my own sign that showed when buses would be arriving near my house so that I could better plan my travels.\nThis type of sign would be very different than what I built for movie showtimes because the data source was completely unknown to me, and also it would need to refresh every minute or so, as opposed to every day. The Inky Frame e-ink screen that I used for the box office sign project is probably not the right hardware to display such a sign because it takes about 20 seconds for the screen to refresh itself, so by the time the new departure times are shown they would already be out-of-date. However I decided to soldier on with wrangling the data and building something for the e-ink sign, and will figure out better hardware to use at a later time."
  },
  {
    "objectID": "posts/realtime-bus-departure-board/index.html#collect_static_details.py",
    "href": "posts/realtime-bus-departure-board/index.html#collect_static_details.py",
    "title": "Making a custom bus departure sign",
    "section": "collect_static_details.py",
    "text": "collect_static_details.py\nThis script downloads the necessary GTFS Static data locally, merges the trips and routes data together, and identifies the bus stops on the specified routes that are nearest to my apartment. The nearest stops are found using the KDTree function from the scipy.spatial package. This algorithm is run for each route and for each direction on each route. The details about the closest stops are then saved locally as a csv file."
  },
  {
    "objectID": "posts/realtime-bus-departure-board/index.html#collect_vehicles_on_route.py",
    "href": "posts/realtime-bus-departure-board/index.html#collect_vehicles_on_route.py",
    "title": "Making a custom bus departure sign",
    "section": "collect_vehicles_on_route.py",
    "text": "collect_vehicles_on_route.py\nThis script uses the GTFS Realtime feed to find vehicles on the specified routes that are currently out in the field. The GTFS Realtime feed is queried using a combination of the google.transit and requests packages. The GTFS Realtime feed returns a lot of information in a json format, but mostly what is needed are elements within the vehicle field. These elements are queried and then restructured into a dataframe and outputted to a csv file."
  },
  {
    "objectID": "posts/realtime-bus-departure-board/index.html#find_vehicles_not_yet_passed.py",
    "href": "posts/realtime-bus-departure-board/index.html#find_vehicles_not_yet_passed.py",
    "title": "Making a custom bus departure sign",
    "section": "find_vehicles_not_yet_passed.py",
    "text": "find_vehicles_not_yet_passed.py\nNow that we have all the active vehicles, we have to filter the list down to those that we are about, namely those that haven’t yet passed by my house on their route. This script simply compares the latitude and lonitude of the bus positions with the latitude and longitude of the bus stops. This works well enough in my case because the routes near my house are quite linear. This would be an issue if there were lots of turns or odd curves in the routes."
  },
  {
    "objectID": "posts/realtime-bus-departure-board/index.html#calculate_eta.py",
    "href": "posts/realtime-bus-departure-board/index.html#calculate_eta.py",
    "title": "Making a custom bus departure sign",
    "section": "calculate_eta.py",
    "text": "calculate_eta.py\nThis is the juiciest bit: among the buses not yet passed by my house, calculate their estimated time of arrival to the stops nearest my house. The script that I ended up with is quite simple, but it oversimplifies some complexities in the data, which definitely leads to more inaccurate calculated ETAs.\nThe core of this script uses the googlemaps python package to query the Directions API to calculate the estimated travel time between the current position of each bus and the bus stop nearest my house. The Directions API returns directions as if you typed the query into Google Maps yourself. If you look for public transit directions, often the directions will first tell you to walk to the transit stop, then get onto the bus, meaning your “trip” will have two “legs”. If you are already at the bus stop, then Google Maps will tell you to just get on the bus, meaning your “trip” will only have one “leg”.\nUsing this Directions API to calculate ETAs for buses already on route is trickier. Google Maps is expecting the query to come from a person not already on bus, but what I am trying to get is travel time as if the person is already on the bus and does not need to walk to a stop. If the bus is currently positioned near a bus stop, then the Directions API will return a “one-leg trip”, and the ETA spit out by the API can be directly taken as the ETA for that bus. If the bus is not currently next to a bus stop, then the API is going to return a “two-leg trip”, with the first leg being “walking” directions to the nearest stop. Somehow I need to convert this “walking time” into “bus travel time” to either add or subtract from the time given by the “transit” step of the trip. You would add or subtract it depending on whether Google Maps is telling you to walk in the direction of travel or against it (your nearest bus stop may be behind you!)\nThe image below shows an example of this. Pretend that the gold star is the current position of a bus, and I am trying to calculate its ETA to the stop nearest my house (off-screen). Google Maps will assume that the gold star is a person who first needs to walk to a bus stop, instead of a bus currently in motion. So for my purposes the distance covered by the “walking step” (blue dotted line) needs to be converted to “transit time” and added to the time given by the “transit step” (red solid line).\n\nIn order to accurately capture this ETA time, I know I would need to do the following:\n\nfind the stop before and the stop after the current position of the bus (i.e., the start of the “walking” step represented by the gold star above)\ndetermine whether walking step is in the direction of travel or not (in the above example, it is in the direction of travel)\nif walking step is in the direction of travel: convert distance to time and add to total ETA time\nif walking step is not in the direction of travel: convert distance to time and subtract from total ETA time\n\n…but I got tripped up on step 1 and couldn’t find a good solution. I can’t use a simple comparison of latitude and longitude here because the routes of the bus are more complicated towards the terminuses (termini?) so the comparison would not be accurate. I can’t try to find the two closest stops to the current position of the bus, because it’s not guaranteed the two closest stops would be in front of and behind the bus. The API itself doesn’t seem to return any data that I can leverage to find the stops before and after the current position.\nSo instead of getting stuck on this point forever, I decided to move on and just take the ETA from the transit step as the total ETA (unless the stop was less than one stop away, in which case I just assumed it was a minute away). Maybe I’ll have an epiphany one day that will help me solve this issue."
  },
  {
    "objectID": "posts/realtime-bus-departure-board/index.html#create_board_image.py",
    "href": "posts/realtime-bus-departure-board/index.html#create_board_image.py",
    "title": "Making a custom bus departure sign",
    "section": "create_board_image.py",
    "text": "create_board_image.py\nNow in the home stretch: the output of the calculate ETA step is read in and manipulated to create a png image displaying all of the active buses and their ETAs. Not very much exciting to say here, except that it was slightly more complicated than the box office sign because I drew shapes around the bus numbers in order to format them like you see on Google Maps."
  },
  {
    "objectID": "posts/realtime-bus-departure-board/index.html#footnotes",
    "href": "posts/realtime-bus-departure-board/index.html#footnotes",
    "title": "Making a custom bus departure sign",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSisterly code review ↩︎\nShout out to our mayor Freddie O’Connell though who is pounding the pavement to get a transit referendum passed that will breathe some much needed life into public transportation here.↩︎"
  },
  {
    "objectID": "posts/power-bi-custom-map/index.html",
    "href": "posts/power-bi-custom-map/index.html",
    "title": "Drawing a custom map for Power BI",
    "section": "",
    "text": "In Power BI it’s fairly easy to generate a map with location data that fits typical geographical boundaries like countries or states. If you want to draw a very custom map, it becomes more difficult. This is a way I figured out that seems much more complicated than it needs to be. I’m sure (I hope) there is an easier way that I’ll discover later.\nCurrently Power BI has multiple built-in mapping visualizations. You can draw custom shapes with the “Shape Map”, but it will be on a blank white background. I wanted my map to lay on top of a typical street map. The ArcGIS visualization makes this possible.\nIf you have an ArcGIS license, you can draw your custom maps within ArcGIS itself and access them via Power BI. This didn’t work for me for a couple reasons: 1. I didn’t have a license, and 2. I don’t know if you can connect your data in Power BI to reference layers you create within ArcGIS. So that makes I need to find a way to have custom location data within Power BI formatted in such a way that it is parseable by the ArcGIS visualization. After lots of digging (ArcGIS’s documentation is lacking), I found that custom shapes can be read from Power BI data in an “EsriJSON format”.\nFirst, I drew my map in Google Earth. Start a new project (it works best in Chrome), and click “New feature” -&gt; “Draw line or shape”.\n\nTrace out your shape, and give it a name. When you’ve finished drawing your map, go to the “…” menu and click “Export as KML file”. I don’t really know what a KML is, but from what I can tell it’s a map format that Google likes to use.\n\nNext, the KML file needs to be converted to a form of JSON called GeoJSON. Again, don’t ask me what a GeoJSON is, but from context clues I’m gonna say it’s a JSON for geographic data.\n\n#open necessary libraries\nlibrary(sf)\nlibrary(tidyverse)\n\n#read in kml file\nmap_geo &lt;- sf::st_read(\"Example Project.kml\")\n\n#spit it back out again as a GeoJSON\nmap_geo %&gt;% \n  st_zm() %&gt;% #remove the Z dimension from the object\n  st_write(\"Example Project.geojson\")\n\nGeoJSON isn’t good enough for finicky ArcGIS/Power BI however, so we need to do a bit more wrangling to create a table that has shape data in an EsriJSON format. This website and this question on the ArcGIS forum helped me figure out what this mysterious EsriJSON needed to look like.\n\n#open necessary libraries\nlibrary(jsonlite)\nlibrary(tidyverse)\n\n#read in geojson file\nlist_json &lt;- read_json(\"Example Project.geojson\")\n\n#wrangle into EsriJSON format\ndf_parsed &lt;- list_json$features %&gt;%\n  map_dfr(.f = function(lst_elm) {\n    lst_elm %&gt;%\n      enframe() %&gt;%\n      pivot_wider()\n  }) %&gt;%\n  unnest_wider(properties) %&gt;%\n  select(-type) %&gt;%\n  unnest_wider(geometry) %&gt;%\n  mutate(coordinates = unlist(coordinates, recursive = FALSE)) %&gt;%\n  mutate(coordinates = map(coordinates, toJSON, digits = 20)) %&gt;%\n  mutate(coordinates = as.character(coordinates)) %&gt;%\n  mutate(coordinates = str_replace(coordinates, \"\\\\]$\", '\\\\]\\\\}')) %&gt;%\n  mutate(coordinates = str_replace(coordinates, \"\\\\[\", '\\\\{\"rings\": \\\\[')) %&gt;%\n  mutate(coordinates = str_replace_all(coordinates, \"\\\\],\\\\[\", \",\"))\n\n\n#print csv with EsriJSON column\nwrite_csv(df_parsed, file = \"Example Project.csv\")\n\nFinally, load this csv file into your Power BI report, put an ArcGIS visualization on your report canvas, and use the coordinates column in the “Location” field to create your custom map.\n\nIf you create relationships between this table and other data in your model, then you can overlay more data on this map via coloring, sizing, and tooltips.\nThat’s it! Easy! I’m gonna go take a nap."
  },
  {
    "objectID": "posts/rstudio-code-snippets/index.html",
    "href": "posts/rstudio-code-snippets/index.html",
    "title": "I love RStudio code snippets",
    "section": "",
    "text": "Just a quick tip that keeps your life organized and saves you the pain of copy/pasting from old scripts: RStudio allows you to make your own custom shortcuts to generate code snippets that you use often. You can see all the code snippets and make your own by going to Tools &gt; Edit Code Snippets in the RStudio IDE. From there you select the file type that the snippet is used for, add/remove/edit the snippets, and hit save.\nI have two main uses for these snippets: to create a header and outline for my R scripts, and to create the YAML header for quarto blog posts (like this one!)\nMy snippet for an R script header looks like this:\n\nsnippet header\n    ## ------------------------------------------------------ ##\n    ##\n    ## Purpose of script: ${1:purpose}\n    ##\n    ## Author: Lindsay Lee\n    ## Email: me@lindsayevanslee.com\n    ##\n    ## Date Created: `r paste(Sys.Date())`\n    ##\n    ##\n    ## ------------------------------------------------------ ##\n    ##\n    ## Notes:\n    ##    - ${2:note}\n    ##   \n    ##\n    ## ------------------------------------------------------ ##\n    \n    \n    \n    \n    ## setup --------------------------------\n    \n    \n    \n    \n    \n    \n    ## read data ----------------------------\n    \n    \n    \n    \n    \n    \n    ## wrangle data -------------------------\n    \n    \n    \n    \n    \n    \n    ## print output -------------------------\n    \n    \n\nEach snippet starts with snippet and then the name of the snippet–in this case, header. Then below it on indented lines you write the code that makes up the snippet. You can also embed variables like I have done for the Purpose and the Notes–these will be areas that your cursor will jump to when the snippet generates. You can also add in-line R code, like I’ve done for the date. Then in any R script you can execute this R script by beginning to type the snippet name, and then hitting Tab when it comes up on the text prediction options:\n\n\n\nText prediction options? Whatever this is called\n\n\nMy snippet for the YAML header for a quarto blog post is like this:\n\nsnippet post\n    ---\n    title: \"\"\n    description: \"\"\n    date: \"\"\n    draft: true\n    #image: \n    image-alt: \"\"\n    categories: []\n    jupyter: python3\n    engine: knitr\n    execute:\n        eval: false\n    ---\n\nThis snippet needs to be saved under the Markdown filetype. Then in any qmd file, type the name of the snippet, hit Shift+Tab and the header will generate! I’ll never need to google this nonsense ever again."
  },
  {
    "objectID": "posts/webscraping-showtimes-update/index.html",
    "href": "posts/webscraping-showtimes-update/index.html",
    "title": "Update on the Inky Frame",
    "section": "",
    "text": "This is a follow-up to my previous post about creating my own movie showtimes board with an inky frame.\nThe big question mark left after I beat the final boss of my inky frame project was how long the screen would continue to work without intervention. To date, here’s what’s happened so far.\nI finished the project on 2/19. On 2/29, the screen got stuck with the “thinking” light left on. I turned it off and back on and then reset the screen by pushing A + E + reset, and then it started working again. I figured this was some network issue that caused it to freeze up.\nI came back from a vacation in March, and saw that the screen was stuck on the showtimes for 3/12. The thinking light wasn’t on this time. I checked the repository, and the showtime images were continuing to generate each day, so the problem was definitely with the screen itself. I tried to turn it on and off again and reset it, but that didn’t fix the issue. This indicated to me the batteries had probably died. I changed the batteries on 3/20 and reset the screen, and it started working again.\nSo this means that the batteries lasted for about 3 weeks worth of hourly refreshes. They weren’t a totally fresh set of batteries because they’re the same ones I used for all my testing, so it’s hard to know exactly how long a totally fresh set would last. I also think that the refresh rate is probably too high, so I’m going to try reducing it to every 2 hours and see how that goes.\n\nUpdate 2024-04-21\nMore dead batteries on 4/17! So it seems like reducing the refresh rate to every 2 hours gave me about another week of screen time, 4 weeks total.\nI’ll try to reduce it further to every 12 hours and see what happens. Start the clock: recharged the batteries and reset the screen at 5:35pm today.\n\n\nnext update\nscreen didn’t refresh on its own until 4/22 at 5:37pm (24 hours later approxiamtely) next morning on 4/23 it was refreshed went on trip, came home afternoon 4/25 and it was refreshed. Saw it refresh again at 5:43. not sure why the first day it seemed to take 24 hours 4/26 saw refresh at 5:45pm"
  },
  {
    "objectID": "posts/apple-calendar-data/index.html",
    "href": "posts/apple-calendar-data/index.html",
    "title": "Analyze data from Apple’s calendar",
    "section": "",
    "text": "I regularly have to count up hours for different events in my calendar, which is a bit tedious. I thought it would be nice to automate this calculation, so with Claude’s help, I wrote a Python script that reads the calendar data from Apple’s Calendar app and counts up the hours for each event. I pushed this script to a GitHub repository and created a GitHub Actions workflow to run every week and display the totals in the Actions summary view. My GitHub repo for this is private, but I’ll show the important pieces here.\nThe packages I use are as follows:\n\nimport caldav\nfrom datetime import datetime, timedelta\nimport pytz\nimport os\nfrom urllib.parse import quote\nimport logging\n\nThe key package is caldav, which is what allows you to connect to the calendar.\nI set up a conda environment with the following environment.yml file:\n\nname: calendar-automation\nchannels:\n  - conda-forge\n  - defaults\ndependencies:\n  - python=3.11\n  - pip=23.2.1\n  - caldav=1.3.9\n  - pytz=2025.2\n\nI created a function to connect to the calendar as follows:\n\ndef connect_to_icloud_calendar():\n    \"\"\"Connect to iCloud calendar via CalDAV\"\"\"\n    \n    # Get credentials from environment variables\n    username = os.getenv('ICLOUD_USERNAME')  # your Apple ID\n    password = os.getenv('ICLOUD_APP_PASSWORD')  # App-specific password\n    \n    if not username or not password:\n        raise ValueError(\"Missing iCloud credentials. Set ICLOUD_USERNAME and ICLOUD_APP_PASSWORD environment variables.\")\n    \n    # iCloud CalDAV URL\n    url = f\"https://{quote(username)}:{quote(password)}@caldav.icloud.com\"\n    \n    try:\n        client = caldav.DAVClient(url)\n        principal = client.principal()\n        calendars = principal.calendars()\n        \n        if not calendars:\n            raise Exception(\"No calendars found\")\n            \n        print(f\"Connected to iCloud calendar successfully\")\n        return calendars\n        \n    except Exception as e:\n        print(f\"Failed to connect to iCloud calendar: {e}\")\n        raise\n\nThis function depends on two environment variables: ICLOUD_USERNAME and ICLOUD_APP_PASSWORD. You can set these variables locally by running this after you have activated your conda environment:\n\nexport ICLOUD_USERNAME=\"your-apple-id@email.com\"\nexport ICLOUD_APP_PASSWORD=\"your-app-specific-password\"\n\nThe app-specific password can be generated by going to appleid.apple.com, logging in, going to the “Sign-In and Security” section, then clicking “App-Specific Passwords”. Follow the instructions to generate an app-specific password. It’ll only show it to you once. You may want to keep a note of it in a password manager so you can use it also for GitHub Actions.\nYou then connect to the calendar by calling the function:\n\ncalendars = connect_to_icloud_calendar()\n\nNext, I wrote a function to get the events from the calendar:\n\ndef get_work_week_events(calendars, event_name, start_date, end_date):\n    \"\"\"Get events within a date range\"\"\"\n    \n    print(f\"Searching for '{event_name}' events from {start_date.date()} ({start_date.strftime('%A')}) to {end_date.date()} ({end_date.strftime('%A')})\")\n    print(\"-\" * 60)\n    \n    matching_events = []\n    total_duration = timedelta(0)\n    \n    # Search through all calendars\n    for calendar in calendars:\n        try:\n            # Fetch events in date range\n            events = calendar.date_search(start=start_date, end=end_date)\n            \n            for event in events:\n                # Parse the event data\n                event_data = event.icalendar_component\n                \n                summary = str(event_data.get('SUMMARY', ''))\n                \n                # Check if event name matches (case insensitive)\n                if event_name.lower() not in summary.lower():\n                    continue\n                \n                # Get event times\n                start_time = event_data.get('DTSTART').dt\n                end_time = event_data.get('DTEND').dt\n                \n                # Handle all-day events (date objects vs datetime objects)\n                if hasattr(start_time, 'hour'):  # It's a datetime\n                    # Convert to UTC if needed\n                    if hasattr(start_time, 'tzinfo') and start_time.tzinfo:\n                        if start_time.tzinfo != pytz.UTC:\n                            start_time = start_time.astimezone(pytz.UTC)\n                    else:\n                        start_time = pytz.UTC.localize(start_time)\n                        \n                    if hasattr(end_time, 'tzinfo') and end_time.tzinfo:\n                        if end_time.tzinfo != pytz.UTC:\n                            end_time = end_time.astimezone(pytz.UTC)\n                    else:\n                        end_time = pytz.UTC.localize(end_time)\n                else:  # It's an all-day event (date object)\n                    # Convert date to datetime at start/end of day\n                    start_time = datetime.combine(start_time, datetime.min.time())\n                    end_time = datetime.combine(end_time, datetime.min.time())\n                    start_time = pytz.UTC.localize(start_time)\n                    end_time = pytz.UTC.localize(end_time)\n                \n                # Calculate duration\n                duration = end_time - start_time\n                \n                matching_events.append({\n                    'summary': summary,\n                    'start': start_time,\n                    'end': end_time,\n                    'duration': duration\n                })\n                total_duration += duration\n                \n        except Exception as e:\n            print(f\"Error processing calendar {calendar}: {e}\")\n            continue\n    \n    # Sort events by start time\n    matching_events.sort(key=lambda x: x['start'])\n    \n    return matching_events, total_duration\n\nThis function takes the calendars data object, the name of the event to search for event_name, and a date range start_date and end_date. It returns a list of matching events and the total duration of those events. It is called like this:\n\nevents, total_duration = get_work_week_events(calendars, event_name, start_date, end_date)\n\nNext I do some processing and formatting of the results, which print to the console when run locally.\nAfter confirming it all works as expected locally, I set up GitHub Actions by creating a workflow file at .github/workflows/weekly-calendar-report.yml:\n\nname: Weekly Calendar Report\n\non:\n  schedule:\n    # Run every Sunday at 9 AM UTC \n    - cron: '0 9 * * 0'  # 0 = Sunday\n  workflow_dispatch: # Allow manual trigger for testing\n\njobs:\n  calendar-report:\n    runs-on: ubuntu-latest\n    \n    steps:\n    - name: Checkout code\n      uses: actions/checkout@v4\n      \n    - name: Setup Conda Environment\n      uses: conda-incubator/setup-miniconda@v3\n      with:\n        environment-file: environment.yml\n        activate-environment: calendar-automation\n        python-version: 3.11\n        auto-activate-base: false\n        auto-update-conda: true\n\n    - name: Check conda environment\n      shell: bash -el {0}  # This ensures conda activation works\n      run: | \n        conda info\n        conda list\n        which python\n        python --version\n\n    - name: Generate Calendar Report\n      shell: bash -el {0}  \n      env:\n        ICLOUD_USERNAME: ${{ secrets.ICLOUD_USERNAME }}\n        ICLOUD_APP_PASSWORD: ${{ secrets.ICLOUD_APP_PASSWORD }}\n        EVENT_NAME: ${{ secrets.EVENT_NAME }}\n      run: |\n        echo \"=== WEEKLY CALENDAR REPORT ===\"\n        echo \"Generated: $(date)\"\n        echo \"Workflow Run: ${{ github.run_id }}\"\n        echo \"\"\n        echo \"Active conda environment: $CONDA_DEFAULT_ENV\"\n        \n        # Run the script and capture output\n        python calendar_report.py &gt; calendar_output.txt 2&gt;&1\n        \n        # Display in logs\n        cat calendar_output.txt\n        \n        # Add to GitHub workflow summary\n        echo \"## 📅 Weekly Calendar Report\" &gt;&gt; $GITHUB_STEP_SUMMARY\n        echo \"**Generated:** $(date)\" &gt;&gt; $GITHUB_STEP_SUMMARY\n        echo \"\" &gt;&gt; $GITHUB_STEP_SUMMARY\n        echo \"### Calendar Output:\" &gt;&gt; $GITHUB_STEP_SUMMARY\n        echo '```' &gt;&gt; $GITHUB_STEP_SUMMARY\n        cat calendar_output.txt &gt;&gt; $GITHUB_STEP_SUMMARY\n        echo '```' &gt;&gt; $GITHUB_STEP_SUMMARY\n        \n        echo \"\"\n        echo \"=== END REPORT ===\"\n\nIn the Generate Calendar Report step, I run the Python script calendar_report.py which contains all the code to connect to the calendar, find the right events, do the analyses, and print the results. The output is captured in a temp file calendar_output.txt, which is then displayed in the GitHub Actions logs and added to the workflow summary by echoing the output to $GITHUB_STEP_SUMMARY.\nThe environmental variables ICLOUD_USERNAME, ICLOUD_APP_PASSWORD, and EVENT_NAME are set as GitHub Secrets, which you can configure in your repository settings under “Settings” -&gt; “Secrets and variables” -&gt; “Actions” -&gt; “Repository secrets”. EVENT_NAME is a pipe-delimited string of event names you want to search for, e.g. Work|Meeting|Project. In my full script I set it up where it searches for all event names included in this string.\nThat’s it! Quite a bit of work to save me about 2 minutes each week, but it’ll be worth it eventually."
  },
  {
    "objectID": "posts/connect-to-gpu/index.html",
    "href": "posts/connect-to-gpu/index.html",
    "title": "Connect to a remote GPU",
    "section": "",
    "text": "I’ve been playing around with some AI stuff, trying to train some models using pytorch (more on this later, stay tuned!). I pretty quickly ran into a road block where my poor lil 2017 MacBook Pro couldn’t take the heat, and I needed to figure out how to set up a virtual machine to run my models on. Here are the steps I took to set up a remote GPU compute engine using the Google Cloud Platform and to connect to it with VS Code."
  },
  {
    "objectID": "posts/connect-to-gpu/index.html#setting-up-cloud-account",
    "href": "posts/connect-to-gpu/index.html#setting-up-cloud-account",
    "title": "Connect to a remote GPU",
    "section": "1. Setting up cloud account",
    "text": "1. Setting up cloud account\nThere are several different cloud providers that have remote development environment options. I chose Google just because I already had a Google Cloud Platform account. First step would be to create that account, and then:\n\nCreate a new project in Google Cloud Platform (GCP)\nGo to “API and Services” → “Enable APIs and Services”\nSearch for “Compute Engine API” and enable it"
  },
  {
    "objectID": "posts/connect-to-gpu/index.html#creating-a-virutal-machine-instance",
    "href": "posts/connect-to-gpu/index.html#creating-a-virutal-machine-instance",
    "title": "Connect to a remote GPU",
    "section": "2. Creating a virutal machine instance",
    "text": "2. Creating a virutal machine instance\n\nIn GCP, go to Compute Engine → VM Instances\nClick “Create Instance” and configure with these specifications:\n\nName: Choose something descriptive (mine is called “vm-old-french-ai”)\nRegion: Choose one close to your location whcih has this type of capacity available\nMachine Configuration:\n\nClick “GPUs”\nSelect series: N1\nGPU: Add 1 NVIDIA T4\nMachine type: n1-standard-4 (4 vCPU, 15GB memory)\n\nBoot disk: Ubuntu 20.04 LTS (100GB)\n\n\nI had to do some trial and error creating a VM. Some errors I got were:\n\nRegion availability: Not all types of VMs are available in all regions. Choose a region that’s as close to you as possible but still has the type of VM you’re choosing\nRegion capacity errors: Some regions have the type of VM available but don’t have capacity at that particular moment. You can try again later or again just try choosing a different region\nGPU quota error: After resolving region errors, I then got an error “The GPUS-ALL-REGIONS-per-project quota maximum has been exceeded.” Click “Request Quota” in the notification, then “Edit Quota”. Request a quota of 1 GPU. Give some details about the project, and then request. This request is reviewed by the Google team. In my case, approval came within minutes."
  },
  {
    "objectID": "posts/connect-to-gpu/index.html#ssh-key-setup",
    "href": "posts/connect-to-gpu/index.html#ssh-key-setup",
    "title": "Connect to a remote GPU",
    "section": "3. SSH key setup",
    "text": "3. SSH key setup\nAfter the VM is set up, you next have to set up the connection to the VM using SSH:\n\nGenerate an SSH key by opening your terminal on your local machine and running:\n\nssh-keygen -t rsa -b 4096 -C \"your-email@example.com\"\nPress Enter to accept the default file location, and optionally set a passphrase.\n\nCopy your public key:\n\ncat ~/.ssh/id_rsa.pub\n\nAdd the key to your VM:\n\nStop the VM instance\nClick on the VM name\nClick “Edit”\nScroll down to “SSH Keys”\nClick “Add Item”\nPaste your public key\n\n\nNote the username at the end of your SSH key (format: YOUR_USERNAME@computer) - you’ll need this later."
  },
  {
    "objectID": "posts/connect-to-gpu/index.html#vs-code-remote-setup",
    "href": "posts/connect-to-gpu/index.html#vs-code-remote-setup",
    "title": "Connect to a remote GPU",
    "section": "4. VS Code remote setup",
    "text": "4. VS Code remote setup\nWe’ll use VS Code as the development environment to connect to the new VM with:\n\nIn VS Code, install Microsoft’s “Remote Development” extension pack\nOpen VS Code command palette (Ctrl/Cmd + Shift + P)\nType “Remote-SSH: Open Configuration File”\nSelect the first configuration file option\nAdd this configuration (replace the placeholders):\n\nHost YOUR_VM_NAME\n    HostName YOUR_EXTERNAL_IP\n    User YOUR_USERNAME\n    IdentityFile ~/.ssh/id_rsa\n\nYOUR_VM_NAME: Whatever you want to call this connection. I used the name of the VM I created in Google\nYOUR_EXTERNAL_IP: Find this in your Google Cloud VM instances list\nYOUR_USERNAME: The username from your SSH key\n\n\nOpen VS Code command palette (Ctrl/Cmd + Shift + P) again and search for “Remote-SSH: Connect to Host”\nSelect the configuration you just created\nIf it asks you anything, say “continue” or “yes”\n\nIf it works without error, you’re in!\nIf it fails to connect, first try connecting via terminal:\nssh -v YOUR_USERNAME@YOUR_EXTERNAL_IP -i ~/.ssh/id_rsa\nAccept the authenticity prompt by typing “yes”. You should see a prompt like this if it worked:\nYOUR_USERNAME@vm-name:~$\nYou can also verify the connection by running this in the remote terminal:\npwd   # Print working directory\nwhoami   # Should show your username\nhostname  # Should show your VM name\nIf this terminal connection works but VS Code doesn’t, close VS Code completely and try again."
  },
  {
    "objectID": "posts/connect-to-gpu/index.html#python-environment-setup",
    "href": "posts/connect-to-gpu/index.html#python-environment-setup",
    "title": "Connect to a remote GPU",
    "section": "5. Python environment setup",
    "text": "5. Python environment setup\nAfter connecting to the VM, now we have to set up the python development environment so that we can actually run stuff.\n\nInstall Miniconda:\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nchmod +x Miniconda3-latest-Linux-x86_64.sh\n./Miniconda3-latest-Linux-x86_64.sh\nFollow the prompts and say ‘yes’ to initialization.\n\nIf you have an existing project with an environment.yml file:\n\nconda env create -f environment.yml\nconda activate YOUR_ENV_NAME"
  },
  {
    "objectID": "posts/connect-to-gpu/index.html#git-setup",
    "href": "posts/connect-to-gpu/index.html#git-setup",
    "title": "Connect to a remote GPU",
    "section": "6. Git setup",
    "text": "6. Git setup\nMy code is hosted on GitHub so I also have to set up git to work with this code on the VM:\n\nInstall git:\n\nsudo apt update\nsudo apt install git -y\n\nConfigure git:\n\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your.email@example.com\"\n\nFor repository access, create a fine-grained Personal Access Token (PAT) on GitHub:\n\nGo to GitHub Settings → Developer Settings → Personal Access Tokens → Fine-grained tokens\nSet Repository Access to “Only select repositories”\nChoose your specific repository\nUnder Permissions → Repository permissions, set:\n\nContents: “Read and write”\nCommit statuses: “Read and write”\nMetadata: “Read-only”\nPull requests: “Read and write”\nWorkflows: “Read and write”\n\n\nClone your repository:\n\ngit clone YOUR_REPO_URL\nUse your GitHub username and PAT when prompted for credentials."
  },
  {
    "objectID": "posts/connect-to-gpu/index.html#nvidia-and-cuda-setup",
    "href": "posts/connect-to-gpu/index.html#nvidia-and-cuda-setup",
    "title": "Connect to a remote GPU",
    "section": "7. NVIDIA and CUDA setup",
    "text": "7. NVIDIA and CUDA setup\nNext we have to install some stuff that lets you use the GPUs. Don’t ask me what a GPU is or what all this does, but it worked:\n\nVerify that we’re on Ubuntu:\n\nlsb_release -a\n\nInstall NVIDIA drivers and CUDA:\n\nsudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt update\nsudo apt install -y ubuntu-drivers-common\nsudo ubuntu-drivers devices    # Shows available drivers\nsudo apt install -y nvidia-driver-560 #Choose the driver from the above command that is listed as recommended\nsudo apt install -y nvidia-cuda-toolkit\n\nReset your VM by going to GCP list of VM instances and clicking “…” -&gt; “Reset”. Verify installation of drivers worked with:\n\nnvidia-smi\nnvcc --version"
  },
  {
    "objectID": "posts/connect-to-gpu/index.html#pytorch-gpu-setup",
    "href": "posts/connect-to-gpu/index.html#pytorch-gpu-setup",
    "title": "Connect to a remote GPU",
    "section": "8. PyTorch GPU setup",
    "text": "8. PyTorch GPU setup\nFinally we have to check that our installation of pytorch is able to access the GPU:\n\nVerify GPU access:\n\npython -c \"import torch; print('PyTorch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('CUDA version PyTorch was built with:', torch.version.cuda); print('GPU device:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')\"\n\nTest GPU functionality:\n\npython -c \"import torch; x = torch.randn(1000, 1000).cuda(); y = torch.randn(1000, 1000).cuda(); z = torch.matmul(x, y); print('Test completed on GPU')\"\nIf you get any failures after running this test code, you may need to fiddle with some versions of packages you’re installing in your environment.yml file. Best advice I can give is to ask ChatGPT/Claude/whatever. This is something that worked for me at one point–uninstalling some package versions and installing other versions with a CUDA toolkit:\nconda remove pytorch torchvision torchaudio  # Remove CPU version if present\nconda install pytorch torchvision torchaudio cudatoolkit=10.1 -c pytorch\nIf you see a version mismatch between CUDA and PyTorch, it should probably be okay as long as these tests run successfully."
  },
  {
    "objectID": "posts/webscraping-showtimes/index.html",
    "href": "posts/webscraping-showtimes/index.html",
    "title": "Making my own box office sign with Scrapy and an Inky Frame",
    "section": "",
    "text": "I’m a flawed human being and therefore susceptible to ads I receive on the internet. One day I was scrolling and stumbled upon this little gadget by Pimoroni: the Inky Frame 7.3”, a fun little e-ink screen that you can program with micropython. I bought it without much a plan for what I would use it for, but thought it would be a fun excuse to practice my python skills. I soon got the idea to combine my loves: I spend all my free time and money at the Belcourt Theatre in Nashville, and thought it would be cool to use the screen as a little display that shows the showtimes for the day.\nAccomplishing this means several things have to happen:\nHere I go through what I did for each of these steps. All the code is available on GitHub."
  },
  {
    "objectID": "posts/webscraping-showtimes/index.html#automate-the-jpg-update",
    "href": "posts/webscraping-showtimes/index.html#automate-the-jpg-update",
    "title": "Making my own box office sign with Scrapy and an Inky Frame",
    "section": "Automate the jpg update",
    "text": "Automate the jpg update\nGitHub Actions can be used to execute scripts on a schedule and make commits to the repository. I used this guide to set up a GitHub Action that would run the spider and generate the jpg every day in the early morning. The syntax for defining the schedule is called cron, and this website is super helpful for figuring out how to configure the cron syntax to do what you want.\nThis GitHub Action requires giving the workflows read/write permissions by enabling: Settings &gt; Actions &gt; General &gt; Workflow Permissions &gt; Read and write permissions. The workflow is defined by a YAML file that I’ve called .github/workflows/actions.yml in the GitHub repository. The script runs the spider, runs the belcourt_generate_image.py script, and then commits the resulting jpg to the repository.\nA second GitHub Action is needed to update the GitHub Pages deployment every time the jpg is updated. By going to Settings &gt; Pages, you can configure the deployment of the webpage. GitHub automatically provides an Action template for you, and there is additional detail in the documentation. One change I made to the YAML file was to schedule the deployment to run after the first action that updates the jpg. I wanted to do this by triggering the deployment after the new changes were committed to the repository. I found this blog that was trying to do the same thing, but I couldn’t get it to work. It’s got something to do with setting SSH keys, which I’ll figure out at a later date. Instead I again used cron to schedule the deployment to run half an hour after the first action, which should be enough time for the jpg generation to finish."
  },
  {
    "objectID": "posts/webscraping-showtimes/index.html#automate-the-inky-frame-refresh",
    "href": "posts/webscraping-showtimes/index.html#automate-the-inky-frame-refresh",
    "title": "Making my own box office sign with Scrapy and an Inky Frame",
    "section": "Automate the Inky Frame refresh",
    "text": "Automate the Inky Frame refresh\nThe final step in this process is to automate the Inky Frame to refresh the image every day at a certain time. This is where I ran into some trouble.\nThe Inky Frame will run anything saved as main.py when it starts up. If main.py is set to:\n\n#run showtimes_from_web.py on start-up\nwith open(\"showtimes_from_web.py\") as f:\n    exec(f.read())\n\nthen my script showtimes_from_web.py will run on start-up, which pulls down the jpg from the GitHub page and displays it on the screen. The screen will start up when it is plugged in to USB power or when the battery pack is turned on. However, I don’t want to have to start up the screen manually each day, and I also don’t want to leave it plugged in all the time, so I need to find a way to get it to refresh itself while on battery power.\nI did a bunch of searching, and there are some helper functions out there that seemed promising.\nI tried a bunch of things first. I tried adding inky_frame.sleep_for(1) to main.py which should theoretically cause it to go to sleep for a minute and then start back up, but that didn’t trigger another run of main.py. I also tried adding this to the showtimes_from_web.py script itself, but that also didn’t work. I tried running another gc.collect() at the end of showtimes_from_web.py in order to ensure that as much of the RAM was available as possible, but that also didn’t work. I tried to find a way to “close” the jpg (the showtimes_from_web.py script has an open_file command, so I figured, maybe it needs to be closed again in order for the script to truly terminate), but it doesn’t seem to be needed because it looks like the decode function called directly after open_file contains a closing function within it.\nIn order to see if the sleep_for() function does wake the frame back up after it sleeps, I tried putting it at the beginning of the main.py function instead of at the end. It does indeed wake back up and continue running main.py. I thought there might be some issue with the showtimes_from_web.py fully finishing and perhaps the sleep_for() script never actually gets executed when it is at the end of main.py. One indicator that the script isn’t fully finishing is that the busy symbol on the screen doesn’t go away after the screen is refreshed.\nI tried following this blog using timers but this also didn’t work. I could see the timer was working but it didn’t spark a refresh, which was more evidence that there is an issue with the showtimes script fully finishing.\nThen I noticed that there was a file on my Inky Frame called state.json that seemed to indicate the script was continuing to run. I tried deleting this file by using inky_helper.clear_state(). the console. After doing this and including sleep_for() at the end of main.py, I was able to trigger a refresh when it was plugged in to USB power! But on battery power, it still didn’t work.\nAfter months of googling and trying new things, I saw that others have posted about having the same sort of issue on both the Pimoroni forums and in their GitHub issues. In that thread someone posts a patch for micropython that they say solves the issue. I tried installing this patch, and it does seem like it prevents the showtimes script from getting stuck and not fully finishing. However, after running sleep_for(), the script just stops and doesn’t start main.py from the beginning as I want.\nI reread the thread on the forum a few times and noticed that the original poster actually executes their graphics.update() and sleep_for() commands within a while loop. When I add this loop to my own script, like this:\n\n#updated main.py\nimport inky_frame\n\nwhile True:\n  \n    with open(\"showtimes_from_web.py\") as f:\n        exec(f.read())\n        \n    inky_frame.sleep_for(1)\n\n…it does successfully cause a refresh after the screen sleeps! We’re getting closer! However after the first refresh, I got a strange EPERM error. After further copying the techniques of the original poster and adding some error logging to my scripts, I saw the error was occurring at the uos.mount() command in showtimes_from_web.py at the second run-through. I found the documentation for uos, and saw that uos.mount() throws this EPERM error when the file system is already mounted. I tried adding a uos.umount() command at the very end of the showtimes_from_web.py script, and this seems to have solved the issue! The screen refreshes continuously!\nFinally, the moment of truth…does it refresh continuously when on battery power? I unplug the frame, turn on the battery pack, perform a reset (by holding down the A, E, and reset buttons), and wait…and it works! The screen refreshes on its own!\n\n\n\n\nIt me\n\n\n\nAll of the scripts that I used on the Inky Frame (these are the only ones loaded–I deleted all other default examples and libraries that it comes with) are in the inky_frame_scripts folder of the GitHub repository. As a last step, I increased the sleep time to 60 minutes with sleep_for(60). Hopefully this doesn’t drain the battery too quickly. I may further increase it later.\nNow I’ll finally be able to sleep at night. Until I come up with another silly idea."
  },
  {
    "objectID": "posts/github-codespaces/index.html",
    "href": "posts/github-codespaces/index.html",
    "title": "Get started with GitHub Codespaces",
    "section": "",
    "text": "After getting started with conda, the next development environment management challenge on my list was GitHub Codespaces. A codespace is a development environment hosted in the cloud, which you can configure with the specific development tools needed for your project. You include the configuration file for the codespace with your repository, so it makes it easy for anyone who is using your code or collaborating with you to work in an environment that reflects yours, preventing difficult-to-diagnose dependency errors.\nThere are various tutorials and templates out there to help you get started, but I still found it difficult to configure my first codespace how I wanted to. There are also things to consider regarding machine size and cost control that I didn’t spend time thinking about and I won’t get in to here. While I still don’t quite understand all the issues I encountered along the way, I landed on a configuration that I think I can adapt and build on for future projects, which installed R and Python and packages for each.\nYou can see your codespaces and initialize a new one in GitHub here. You can also initialize a codespace directly from a repository. I built my first codespace in the repository babys-first-codespace. I first created the repository, then from the repository clicked Code -&gt; Codespaces -&gt; Create codespace on main. Then you are taken to Visual Studio Code in the browser. You can see your code is running on a virtual machine from the codespace name in the bottom left corner of the window.\nInitializing a codespace this way starts you off with a completely blank slate, and at first it wasn’t very clear to me what in the world you’re supposed to do next. After some googling, I saw you can search for templates by opening the Command Palette (Cmd+Shift+P) and hitting “Codespaces: Add Dev Container Configuration Files…” then “Create a new configuration”. There is one template that mentions R and Python called Data Science with Python and R. I had issues adding this template directly to the repository this way, but instead I manually copied the src/datascience-py-r folder in the template’s repository into my repository. The key file is .devcontainer/devcontainer.json, which holds all the configurations for the codespace.\nI could see from this file that there’s a step \"postCreateCommand\" that mentions installing python packages from a requirements.txt file. I added this file to my repository, specified a few packages, and rebuilt the codespace (open the Command Palette and search for “Codespaces: Rebuild Container” if you aren’t prompted to rebuild automatically after making changes to devcontainer.json). This seemed to install the python packages I wanted without an issue. One coding language down, one to go!\nThis devcontainer.json template uses the features r-apt to install R and apt-packages to install some R packages. I tried adding more R packages to the list for apt-packages but ran into some issues. After more googling I found this Stack Overflow response that indicated that not all CRAN packages are available via apt. I tried installing only packages I could find on the Ubuntu Packages Search, and that seemed to work.\nNext I tried adding the r-packages feature to install other CRAN packages, but I couldn’t get it to work. The error message that showed up in creation.log after a failed container rebuild was vague.\nI found this blog post where someone shared the devcontainer.json they use for developing with Quarto. They use a rocker/tidyverse image as a base. I hoped that by using this image instead, I could install the R packages I need with the r-packages feature and still use the python feature. After updating the key parts of my devcontainer.json script to look like this…\n\n...\n\"image\": \"ghcr.io/rocker-org/devcontainer/tidyverse:4.3\",\n// Features to add to the dev container. More info: https://containers.dev/features.\n\"features\": {\n        \"ghcr.io/devcontainers/features/python:1\": {\n            \"version\": \"latest\"\n        },\n        \"ghcr.io/rocker-org/devcontainer-features/r-packages:1\": {\n            \"packages\": \"blogdown\",\n            \"installSystemRequirements\" : true\n        }\n    },\n    // Use 'postCreateCommand' to run commands after the container is created.\n    \"postCreateCommand\": \"pip install ipykernel ipywidgets && if [ -f requirements.txt* ]; then pip install -r requirements.txt; else pip install pandas numpy matplotlib seaborn scikit-learn; fi\",\n...\n\n…I still got an error. But a different one! With the help of GitHub Copilot, I figured out it was something about not being able to find the remoteUser of \"vscode\". I commented out the remoteUser specification (who knows what this does anyway) in the template and rebuilt the container, and the error went away!\nWe did it! A minimally useful GitHub Codespace! This codespace is not particularly sophisticated, but I think knowing how to set one of these up is going to become more and more important as more development is pushed to the cloud in the age of AI (don’t I sound smart?). I know for me at least I’m pushing my 7 year old Macbook to the limit trying to run AI stuff locally (blog post forthcoming, hopefully). Running on a codespace is going to give my lil laptop some relief."
  },
  {
    "objectID": "posts/font-design/index.html",
    "href": "posts/font-design/index.html",
    "title": "Designing a custom font (with emojis)",
    "section": "",
    "text": "Inspired by my sister’s awesome PhD work in Medieval French, I have been playing around with designing a custom font. My latest experimentation is in this font-test GitHub repo and is published to an accompanying GitHub pages site, which you can see here:"
  },
  {
    "objectID": "posts/font-design/index.html#software",
    "href": "posts/font-design/index.html#software",
    "title": "Designing a custom font (with emojis)",
    "section": "Software",
    "text": "Software\nI am using Glyphs 3 for creating this font. Glyphs is a proprietary font design program for Mac that seems to be one of the most popular ones. They have student pricing and also a “mini” version that isn’t as expensive. I think you could do basically everything I have done so far with the mini version, except for the emojis.\nI am using Flow for sketching glyphs."
  },
  {
    "objectID": "posts/font-design/index.html#some-references-that-helped-me",
    "href": "posts/font-design/index.html#some-references-that-helped-me",
    "title": "Designing a custom font (with emojis)",
    "section": "Some references that helped me",
    "text": "Some references that helped me\nGlyphs has some pretty good documentation, and there are also some good videos on YouTube to help you get started:\n\nDrawing good paths\nDanielNisbet on Youtube\nCreating an Apple color font\nVertical metrics"
  },
  {
    "objectID": "posts/font-design/index.html#steps",
    "href": "posts/font-design/index.html#steps",
    "title": "Designing a custom font (with emojis)",
    "section": "Steps",
    "text": "Steps\n\nGetting started\n\nCreate new font in Glyphs\nSet parameters under Font Info: Units per Em, Ascender, Descender, etc\n\n\n\nLower case letters\n\nMake new components to share between glyphs by creating new glyphs with names starting with _\nDraw the letters, and right click to add components\n\n\n\nUpper case letters\n\nSet custom Exports parameters:\n\n\nDisable Subroutines (on): needed because the capitals are very complicated\n\n\nCopy sketches into glyph, and set size\nUse “Trace Image” plugin to trace the image\nClean up\nDelete image\n\n\n\nEmojis\n\nSet custom Font parameters:\n\n\nUse Typo Metrics (on)\n\n\nSet custom Masters parameters:\n\n\nhheaAscender, hheaDescender, hheaLineGap: needed to avoid clipping\n\n\nSet custom Export parameters:\n\n\nExport sbix Table: needed for emojis to render\nSBIX to SVG: needed to increase support for color font\n\n\nCreate new glyph called “_emoji_origin”, set width to be the same as the UPM for the font, and put small path in the bottom left corner where the emoji’s bottom left corner will be anchored\nCreate new glyph with the emoji needed (either generate from glyph list or add directly by using the exact name or unicode)\nAdd component _emoji_origin to emoji and decompose\nPrepare image files for emoji:\n\n\nOriginal image should be square png with transparent background\nCreate a copies of images with different resolutions. For example set first image as resolution 512, with a height and width that works when you paste image into the glyph. Next image should have half resolution (256) and half height and width\nSet names of files as all “glyphname resolution.png”, eg “slightlySmilingFace 512.png”\n\n\nUse script Add sbix Images to Font to batch-import your emoji images\n\n\nThis is a mekkablue script that can be added from Plugin Manager -&gt; Scripts\nIf it doesn’t work, you can read error messages by opening the Macro Panel"
  },
  {
    "objectID": "posts/font-design/index.html#next-steps",
    "href": "posts/font-design/index.html#next-steps",
    "title": "Designing a custom font (with emojis)",
    "section": "Next steps",
    "text": "Next steps\n\nMake the font more cohesive\nDraw better\nMake more emojis\nFigure out something cool to do with this"
  },
  {
    "objectID": "posts/webscraping-screenshots/index.html",
    "href": "posts/webscraping-screenshots/index.html",
    "title": "Webscraping screenshots with Selenium",
    "section": "",
    "text": "My team at work maintained a dashboard related to COVID-19 that included screenshots of graphs online that needed to be updated daily. I had recently taken a little LinkedIn Learning course called Web Scraping with Python and thought this could be a cool opportunity to try that out. The course doesn’t really talk about webscraping screenshots, but it helped give me some of the foundational knowledge and vocabulary.\nAfter a lot of trial and error, here is an small script I came up with. This script cycles through a python dictionary of directions to specific locations, captures the screenshot, and saves it. One element of the dictionary contains the URL where the screenshot is, an XPATH direction to the location of the screenshot on the page, the frame number, and the name of the file that it should be saved to. The XPATH may give you a list of multiple web elements (or “frames”), so the frame number is needed to tell the program which element of that list to save.\nThis script doesn’t work exactly anymore, because these web pages have changed. That’s one major drawback: it’s very unstable and sensitive to change. And because of that, we didn’t end up implementing this at work. But this same script structure could be used for different purposes in the future.\n\n##import packages -----------\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom time import sleep\nfrom datetime import date\n\n\n## set up --------\n\n#open driver\ndriver = webdriver.Safari()\n\n#create function to see full page\nS = lambda X: driver.execute_script('return document.body.parentNode.scroll'+X)\n\n#get today's date\ntoday = date.today().strftime(\"%Y-%m-%d\")\n\n#dictionary of screenshots to capture\ndict_urls = [\n    {\n        \"id\": 1, \n        \"url\": 'https://insight.livestories.com/s/v2/1-2-case-counts/c4f65175-2433-47b7-b112-d62cf719af71',\n        \"xpath\": \"//main//iframe\",\n        \"frame_number\": 11,\n        \"filename\": 'weekly-covid-19-test-positivity-rate'\n    },\n        {\n        \"id\": 2, \n        \"url\": 'https://insight.livestories.com/s/v2/1-4-geographic-data/6bb3072d-e622-4b84-9555-7b0ef390b354',\n        \"xpath\": \"//main//iframe\",\n        \"frame_number\": 0,\n        \"filename\": '14-day-testing-rate-per-100000-pop'\n    },\n        {\n        \"id\": 3, \n        \"url\": 'https://insight.livestories.com/s/v2/1-4-geographic-data/6bb3072d-e622-4b84-9555-7b0ef390b354',\n        \"xpath\": \"//main//iframe\",\n        \"frame_number\": 1,\n        \"filename\": '14-day-case-rate-per-100000-pop'\n    },\n        {\n        \"id\": 4, \n        \"url\": 'https://insight-editor.livestories.com/s/v2/1.1-data-dashboard/5d1c9c7a-1eb4-4e9c-82ab-efeaa6258cad',\n        \"xpath\": \"//div[contains(@class, 'css-1bedmrb') and contains(@class, 'erxya8v2')]/div[contains(@class, 'css-rpv578') and contains(@class, 'ezdhjma0')]\",\n        \"frame_number\": 9,\n        \"filename\": 'hrts'\n    }\n    ]\n\n\n## screenshots ------------------------\n\n## loop through dictionary of screenshots\nfor i in range(len(dict_urls)):\n    #go to URL\n    driver.get(url = dict_urls[i]['url'])\n    sleep(3)\n\n    #set window size to full page\n    driver.set_window_size(S('Width'),S('Height')) # May need manual adjustment   \n    sleep(3)\n\n    #find set of frames\n    myframe = driver.find_elements(By.XPATH, dict_urls[i]['xpath'])\n\n    #print needed figure\n    myframe[dict_urls[i]['frame_number']].screenshot(dict_urls[i]['filename'] + \"_\" + today + '.png')\n\n## close driver ----------------------------------\ndriver.quit()\nprint(\"end...\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A blog of technical trial and error",
    "section": "",
    "text": "Analyze data from Apple’s calendar\n\n\n\n\n\n\ncalendar\n\n\npython\n\n\ngithub\n\n\nautomation\n\n\n\nNew automation alert\n\n\n\n\n\nMay 26, 2025\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nConnect to a remote GPU\n\n\n\n\n\n\ngpu\n\n\nai\n\n\ngoogle cloud\n\n\n\nBring out the big guns\n\n\n\n\n\nNov 3, 2024\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nDownloading newspapers from the Library of Congress\n\n\n\n\n\n\nwebscraping\n\n\npython\n\n\n\nLeveling up my webscraping game\n\n\n\n\n\nOct 19, 2024\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nGoing live with Shinylive\n\n\n\n\n\n\nshiny\n\n\nweb app\n\n\npython\n\n\nquarto\n\n\n\nAm I a web developer?\n\n\n\n\n\nAug 1, 2024\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nMaking a custom bus departure sign\n\n\n\n\n\n\npython\n\n\nraspberry pi\n\n\ngoogle maps\n\n\n\nNashville’s transit system is bad but its data is not!\n\n\n\n\n\nJul 7, 2024\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nGet started with GitHub Codespaces\n\n\n\n\n\n\ngithub\n\n\nworkflow\n\n\n\nI’m on such a development-environment-manangement roll lately\n\n\n\n\n\nJun 9, 2024\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nGet started with conda\n\n\n\n\n\n\npython\n\n\nconda\n\n\nworkflow\n\n\n\nI’ve put off learning this for too long\n\n\n\n\n\nApr 5, 2024\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nI love RStudio code snippets\n\n\n\n\n\n\nrstudio\n\n\nr\n\n\nquarto\n\n\nworkflow\n\n\n\nI don’t love copy/pasting\n\n\n\n\n\nMar 31, 2024\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyze health data from the Apple Watch\n\n\n\n\n\n\nhealth\n\n\napple watch\n\n\nr\n\n\npython\n\n\n\nBeing #extra at my doctor’s appointment\n\n\n\n\n\nMar 30, 2024\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate on the Inky Frame\n\n\n\n\n\n\nwebscraping\n\n\npython\n\n\nraspberry pi\n\n\nautomation\n\n\n\nSpoiler alert: the batteries died\n\n\n\n\n\nMar 21, 2024\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nMaking my own box office sign with Scrapy and an Inky Frame\n\n\n\n\n\n\nwebscraping\n\n\npython\n\n\nraspberry pi\n\n\nautomation\n\n\n\nThe algorithm got me again\n\n\n\n\n\nFeb 19, 2024\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nDrawing a custom map for Power BI\n\n\n\n\n\n\nmaps\n\n\npower bi\n\n\nr\n\n\n\nIt took like 4 different programs but we did it.\n\n\n\n\n\nMar 26, 2023\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning a custom font (with emojis)\n\n\n\n\n\n\nfonts\n\n\n\nInspired by my sister’s awesome PhD work in Medieval French, I have been playing around with designing a custom font.\n\n\n\n\n\nMar 26, 2023\n\n\nLindsay Lee\n\n\n\n\n\n\n\n\n\n\n\n\nWebscraping screenshots with Selenium\n\n\n\n\n\n\nwebscraping\n\n\npython\n\n\n\nNo more repetitive copy-pasting!\n\n\n\n\n\nMar 26, 2023\n\n\nLindsay Lee\n\n\n\n\n\n\nNo matching items"
  }
]